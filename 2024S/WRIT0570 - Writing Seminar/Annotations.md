---
date: 2024-02-24
type: note
tags: 
---

# Article 1
[The hidden costs of AI: Impending energy and resource strain | Penn Today](https://penntoday.upenn.edu/news/hidden-costs-ai-impending-energy-and-resource-strain)

This article focuses on the energy and resource costs that AI computing will bring in the near-term future. According to Professor Deep Jariwala and Benjamin Lee at the University of Pennsylvania, AI will bring about massive challenges in memory and energy usage. This is useful for my research because it confirms that there is a problem in accelerating AI developments, and that experts in AI, computing, and electrical engineering agree that this a major problem that is being under looked.

Intro:
- As AI become more elaborate and data-intensive, two things begin to scale up exponentially: the need for more memory storage and the need for more energy.
- The rise of AI, Big Data, and cloud computing has led to significant growth in data centers.
- Recent NVIDIA earnings' report state a 404% growth in data center revenues within the past 10 years.
- Moore’s law states that the number of transistors on a chip—the parts that control the flow of electrons on a semiconductor material—doubles every two or so years.
- Dennard’s scaling suggests that doubling the number of transistors effectively means shrinking them but also maintaining their power density, so smaller chips meant more energy-efficient chips.

Problem:
- Regarding memory, an estimate from the Semiconductor Research Corporation, a consortium of all the major semiconductor companies, posits that if we continue to scale data at this rate, which is stored on memory made from silicon, we will outpace the global amount of silicon produced every year. So, pretty soon we will hit a wall where our silicon supply chains won’t be able to keep up with the amount of data being generated.
- Couple this with the fact that in 2018 our computers consumed roughly 1-2% of the global electricity supply, and in 2020, this figure was estimated to be around 4–6%. If we continue at this rate, by 2030, it's projected to rise between 8-21%, further exacerbating the current energy crisis.
- Companies like Amazon, Google, and Meta have been building more and more of these massive facilities all over the country. In fact, data center power and carbon emissions associated with data centers doubled between 2017 and 2020. Each facility consumes in the order of 20 megawatts up to 40 megawatts of power, and most of the time data centers are running at 100% utilization, meaning all the processors are being kept busy with some work. So, a 20-megawatt facility probably draws 20 megawatts fairly consistently—enough to power roughly 16,000 households.
- Then there’s the embodied carbon footprint, which is associated with construction and manufacturing. This hearkens back to building new semiconductor foundries and packaging all the chips we’ll need to produce to keep up with increasing compute demand. These processes in and of themselves are extremely energy-intensive, expensive and have a carbon impact at each step.
- Moore's Law and Dennard's scaling have started to slow down for several reasons related to the physical limits of the materials we use.

Solutions:
- Each computational task is a transaction between memory and processing that requires some energy. Deep Jariwala's lab is trying to figure out ways to make each operation use fewer watts of power. One way to reduce this metric is through tightly integrating memory and processing units because these currently exist in two separate locations that are millimeters to centimeters apart so electricity needs to travel great distances to facilitate computation which makes it energy and time inefficient. We call it vertically heterogenous-integrated architecture, and developing this is key to reducing energy consumption.

Conclusions:
- Our computers and other devices are becoming insatiable energy beasts that we continue to feed. That’s not to say AI and advancing it needs to stop because it’s incredibly useful for important applications like accelerating the discovery of therapeutics. We just need to remain cognizant of the effects and keep pushing for more sustainable approaches to design, manufacturing, and consumption.

# Article 2
[Brain organoid reservoir computing for artificial intelligence | Nature Electronics](https://www.nature.com/articles/s41928-023-01069-w)

This article focuses on a new method of computing: using biology to create biologics-based computing. This method will bring about a new form of energy-efficiency and power-efficiency previously unseen in traditional silicon chips. This is useful for my research because it is 1 very interesting and unique method of solving this problem of creating more energy-efficient, power-efficient, and climate friendly semiconductor chip. This will be 1 of the 3 major ways to tackle the issue I am exploring in my white paper: "Current methods and solutions to solving the energy demands caused by the explosive growth of AI".



# Article 3
[Reservoir computing with brain organoids | Nature Electronics](https://www.nature.com/articles/s41928-023-01096-7)

# Article 4
[How thermal management is changing in the 1 kW chip age • The Register](https://www.theregister.com/2023/12/26/thermal_management_is_changing/)

# Article 5
[Pathways to cellular supremacy in biocomputing | Nature Communications](https://www.nature.com/articles/s41467-019-13232-z)

# Article 6
[Computing Power and the Governance of AI | GovAI Blog](https://www.governance.ai/post/computing-power-and-the-governance-of-ai)

# Article 7
[Navigating the High Cost of AI Compute | Andreessen Horowitz](https://a16z.com/navigating-the-high-cost-of-ai-compute/)